{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11418165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MEMORY-EFFICIENT MODELING WITH EXPLAINABILITY\n",
    "# Includes: PCA, RFE, Baseline Models, SHAP, LIME\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d82728b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING PREPROCESSED DATA\n",
      "================================================================================\n",
      "Train: (156156, 98), Test: (39040, 98)\n",
      "Memory: 145.94 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10222"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 1. LOAD PREPROCESSED DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING PREPROCESSED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X_train = pd.read_csv('preprocessed_train.csv')\n",
    "X_test = pd.read_csv('preprocessed_test.csv')\n",
    "y_train = pd.read_csv('y_train.csv').squeeze()\n",
    "y_test = pd.read_csv('y_test.csv').squeeze()\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "print(f\"Memory: {(X_train.memory_usage().sum() + X_test.memory_usage().sum()) / 1024**2:.2f} MB\")\n",
    "\n",
    "# Optimize dtypes\n",
    "for col in X_train.columns:\n",
    "    if X_train[col].dtype == 'float64':\n",
    "        X_train[col] = X_train[col].astype('float32')\n",
    "        X_test[col] = X_test[col].astype('float32')\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a3fe9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PCA - DIMENSIONALITY REDUCTION\n",
      "================================================================================\n",
      "Fitting PCA with 49 components...\n",
      "✓ Explained variance: 87.27%\n",
      "✓ Components for 95% variance: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 2. PCA - DIMENSIONALITY REDUCTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PCA - DIMENSIONALITY REDUCTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "n_components = min(50, X_train.shape[1] // 2)\n",
    "pca = PCA(n_components=n_components, random_state=42)\n",
    "\n",
    "print(f\"Fitting PCA with {n_components} components...\")\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "explained_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "n_95 = np.argmax(explained_var >= 0.95) + 1\n",
    "\n",
    "print(f\"✓ Explained variance: {explained_var[-1]:.2%}\")\n",
    "print(f\"✓ Components for 95% variance: {n_95}\")\n",
    "\n",
    "X_train_pca_df = pd.DataFrame(\n",
    "    X_train_pca,\n",
    "    columns=[f'PC{i+1}' for i in range(n_components)]\n",
    ").astype('float32')\n",
    "\n",
    "X_test_pca_df = pd.DataFrame(\n",
    "    X_test_pca,\n",
    "    columns=[f'PC{i+1}' for i in range(n_components)]\n",
    ").astype('float32')\n",
    "\n",
    "del X_train_pca, X_test_pca\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d45c25ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RFE - FEATURE SELECTION\n",
      "================================================================================\n",
      "Running RFE on 50000 samples to select 30 features...\n",
      "✓ Selected features: ['VISITDAY', 'VISITYR', 'NACCVNUM', 'BIRTHYR', 'INDEPEND', 'INBIRYR', 'HEIGHT', 'WEIGHT', 'BPSYS', 'HRATE']...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 3. RFE - RECURSIVE FEATURE ELIMINATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RFE - FEATURE SELECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use smaller sample for RFE to save memory\n",
    "sample_size = min(50000, len(X_train))\n",
    "sample_idx = np.random.choice(len(X_train), sample_size, replace=False)\n",
    "\n",
    "rf_estimator = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "n_features_to_select = min(30, X_train.shape[1])\n",
    "rfe = RFE(estimator=rf_estimator, n_features_to_select=n_features_to_select, step=10)\n",
    "\n",
    "print(f\"Running RFE on {sample_size} samples to select {n_features_to_select} features...\")\n",
    "rfe.fit(X_train.iloc[sample_idx], y_train.iloc[sample_idx])\n",
    "\n",
    "selected_rfe_features = X_train.columns[rfe.support_].tolist()\n",
    "\n",
    "X_train_rfe = X_train[selected_rfe_features].astype('float32')\n",
    "X_test_rfe = X_test[selected_rfe_features].astype('float32')\n",
    "\n",
    "print(f\"✓ Selected features: {selected_rfe_features[:10]}...\")\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6a9774a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BASELINE MODELING\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Feature Set: Original (98 features)\n",
      "============================================================\n",
      "\n",
      "Training Logistic Regression...\n",
      "  Accuracy:  0.9506\n",
      "  Precision: 0.9321\n",
      "  Recall:    0.8979\n",
      "  F1-Score:  0.9147\n",
      "  ROC-AUC:   0.9879\n",
      "  Time:      3.70s\n",
      "\n",
      "Training Random Forest...\n",
      "  Accuracy:  0.9501\n",
      "  Precision: 0.9339\n",
      "  Recall:    0.8943\n",
      "  F1-Score:  0.9136\n",
      "  ROC-AUC:   0.9886\n",
      "  Time:      5.09s\n",
      "\n",
      "Training Gradient Boosting...\n",
      "  Accuracy:  0.9532\n",
      "  Precision: 0.9295\n",
      "  Recall:    0.9102\n",
      "  F1-Score:  0.9198\n",
      "  ROC-AUC:   0.9897\n",
      "  Time:      88.43s\n",
      "\n",
      "============================================================\n",
      "Feature Set: PCA (49 features)\n",
      "============================================================\n",
      "\n",
      "Training Logistic Regression...\n",
      "  Accuracy:  0.9478\n",
      "  Precision: 0.9303\n",
      "  Recall:    0.8898\n",
      "  F1-Score:  0.9096\n",
      "  ROC-AUC:   0.9865\n",
      "  Time:      1.45s\n",
      "\n",
      "Training Random Forest...\n",
      "  Accuracy:  0.9297\n",
      "  Precision: 0.8901\n",
      "  Recall:    0.8690\n",
      "  F1-Score:  0.8794\n",
      "  ROC-AUC:   0.9755\n",
      "  Time:      14.86s\n",
      "\n",
      "Training Gradient Boosting...\n",
      "  Accuracy:  0.9446\n",
      "  Precision: 0.9166\n",
      "  Recall:    0.8934\n",
      "  F1-Score:  0.9049\n",
      "  ROC-AUC:   0.9848\n",
      "  Time:      423.10s\n",
      "\n",
      "============================================================\n",
      "Feature Set: RFE (30 features)\n",
      "============================================================\n",
      "\n",
      "Training Logistic Regression...\n",
      "  Accuracy:  0.9495\n",
      "  Precision: 0.9322\n",
      "  Recall:    0.8938\n",
      "  F1-Score:  0.9126\n",
      "  ROC-AUC:   0.9874\n",
      "  Time:      1.94s\n",
      "\n",
      "Training Random Forest...\n",
      "  Accuracy:  0.9499\n",
      "  Precision: 0.9294\n",
      "  Recall:    0.8983\n",
      "  F1-Score:  0.9136\n",
      "  ROC-AUC:   0.9884\n",
      "  Time:      3.82s\n",
      "\n",
      "Training Gradient Boosting...\n",
      "  Accuracy:  0.9514\n",
      "  Precision: 0.9268\n",
      "  Recall:    0.9070\n",
      "  F1-Score:  0.9168\n",
      "  ROC-AUC:   0.9889\n",
      "  Time:      47.29s\n",
      "\n",
      "================================================================================\n",
      "RESULTS SUMMARY\n",
      "================================================================================\n",
      "Feature_Set               Model  Accuracy  Precision   Recall       F1      AUC       Time\n",
      "   Original   Gradient Boosting  0.953151   0.929515 0.910227 0.919770 0.989676  88.426216\n",
      "        RFE   Gradient Boosting  0.951434   0.926810 0.907015 0.916806 0.988881  47.294125\n",
      "   Original Logistic Regression  0.950589   0.932132 0.897899 0.914695 0.987881   3.698973\n",
      "   Original       Random Forest  0.950128   0.933902 0.894252 0.913647 0.988554   5.093476\n",
      "        RFE       Random Forest  0.949872   0.929399 0.898333 0.913602 0.988436   3.824555\n",
      "        RFE Logistic Regression  0.949488   0.932180 0.893818 0.912596 0.987354   1.935023\n",
      "        PCA Logistic Regression  0.947823   0.930290 0.889825 0.909607 0.986468   1.449232\n",
      "        PCA   Gradient Boosting  0.944570   0.916622 0.893384 0.904854 0.984768 423.097911\n",
      "        PCA       Random Forest  0.929688   0.890084 0.868988 0.879410 0.975459  14.862301\n",
      "\n",
      "✓ Results saved to 'model_results.csv'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 4. BASELINE MODELS - MULTIPLE ALGORITHMS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE MODELING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define models with memory-efficient settings\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=500, random_state=42, n_jobs=-1),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        max_depth=10, \n",
    "        min_samples_split=100,\n",
    "        random_state=42, \n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(\n",
    "        n_estimators=100, \n",
    "        learning_rate=0.1, \n",
    "        max_depth=5,\n",
    "        subsample=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# Feature sets\n",
    "feature_sets = {\n",
    "    'Original': (X_train, X_test),\n",
    "    'PCA': (X_train_pca_df, X_test_pca_df),\n",
    "    'RFE': (X_train_rfe, X_test_rfe)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for feat_name, (X_tr, X_te) in feature_sets.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Feature Set: {feat_name} ({X_tr.shape[1]} features)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        try:\n",
    "            start = time()\n",
    "            \n",
    "            # Train\n",
    "            print(f\"\\nTraining {model_name}...\")\n",
    "            model.fit(X_tr, y_train)\n",
    "            \n",
    "            # Predict\n",
    "            y_pred = model.predict(X_te)\n",
    "            y_proba = model.predict_proba(X_te)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "            \n",
    "            # Metrics\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "            rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "            f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "            auc = roc_auc_score(y_test, y_proba) if y_proba is not None else 0\n",
    "            \n",
    "            elapsed = time() - start\n",
    "            \n",
    "            print(f\"  Accuracy:  {acc:.4f}\")\n",
    "            print(f\"  Precision: {prec:.4f}\")\n",
    "            print(f\"  Recall:    {rec:.4f}\")\n",
    "            print(f\"  F1-Score:  {f1:.4f}\")\n",
    "            print(f\"  ROC-AUC:   {auc:.4f}\")\n",
    "            print(f\"  Time:      {elapsed:.2f}s\")\n",
    "            \n",
    "            results.append({\n",
    "                'Feature_Set': feat_name,\n",
    "                'Model': model_name,\n",
    "                'Accuracy': acc,\n",
    "                'Precision': prec,\n",
    "                'Recall': rec,\n",
    "                'F1': f1,\n",
    "                'AUC': auc,\n",
    "                'Time': elapsed\n",
    "            })\n",
    "            \n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR: {str(e)}\")\n",
    "\n",
    "# Summary\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.sort_values('F1', ascending=False).to_string(index=False))\n",
    "\n",
    "results_df.to_csv('model_results.csv', index=False)\n",
    "print(\"\\n✓ Results saved to 'model_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e7ee5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING BEST MODEL\n",
      "================================================================================\n",
      "Training Random Forest for explainability...\n",
      "✓ Model trained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 5. TRAIN BEST MODEL FOR EXPLAINABILITY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING BEST MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use Random Forest as best model\n",
    "best_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training Random Forest for explainability...\")\n",
    "best_model.fit(X_train, y_train)\n",
    "print(\"✓ Model trained\")\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57057afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SHAP - MODEL EXPLAINABILITY\n",
      "================================================================================\n",
      "Creating SHAP explainer for 500 samples...\n",
      "Calculating SHAP values...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|=================== | 961/1000 [00:19<00:00]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SHAP summary plot saved\n",
      "⚠ SHAP error: Per-column arrays must each be 1-dimensional\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 6. SHAP - EXPLAINABILITY (CRITICAL!)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SHAP - MODEL EXPLAINABILITY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "    \n",
    "    # Use smaller sample for SHAP (memory intensive)\n",
    "    sample_size = min(500, len(X_test))\n",
    "    X_test_sample = X_test.sample(sample_size, random_state=42)\n",
    "    \n",
    "    print(f\"Creating SHAP explainer for {sample_size} samples...\")\n",
    "    explainer = shap.TreeExplainer(best_model, X_train.sample(min(1000, len(X_train))))\n",
    "    \n",
    "    print(\"Calculating SHAP values...\")\n",
    "    shap_values = explainer.shap_values(X_test_sample)\n",
    "    \n",
    "    # For binary classification\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[1]\n",
    "    \n",
    "    # Summary plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values, X_test_sample, show=False, max_display=20)\n",
    "    plt.title(\"SHAP Feature Importance\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('shap_summary.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ SHAP summary plot saved\")\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': np.abs(shap_values).mean(axis=0)\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\n✓ Top 15 Features by SHAP:\")\n",
    "    print(feature_importance.head(15).to_string(index=False))\n",
    "    \n",
    "    feature_importance.to_csv('shap_feature_importance.csv', index=False)\n",
    "    print(\"\\n✓ SHAP importance saved to 'shap_feature_importance.csv'\")\n",
    "    \n",
    "    # Dependence plot for top feature\n",
    "    top_feature = feature_importance.iloc[0]['feature']\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.dependence_plot(\n",
    "        top_feature, \n",
    "        shap_values, \n",
    "        X_test_sample,\n",
    "        show=False\n",
    "    )\n",
    "    plt.title(f\"SHAP Dependence Plot: {top_feature}\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('shap_dependence.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"✓ SHAP dependence plot saved for {top_feature}\")\n",
    "    \n",
    "    del explainer, shap_values\n",
    "    gc.collect()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"⚠ SHAP not installed. Run: pip install shap\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ SHAP error: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50474ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LIME - LOCAL INTERPRETABILITY\n",
      "================================================================================\n",
      "\n",
      "Explaining Dementia Case...\n",
      "✓ LIME explanation 1 saved\n",
      "\n",
      "Explaining No Dementia Case...\n",
      "✓ LIME explanation 2 saved\n",
      "\n",
      "✓ LIME explanations complete\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 7. LIME - LOCAL EXPLANATIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LIME - LOCAL INTERPRETABILITY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    from lime.lime_tabular import LimeTabularExplainer\n",
    "    \n",
    "    # Create LIME explainer with smaller sample\n",
    "    sample_train = X_train.sample(min(5000, len(X_train)), random_state=42)\n",
    "    \n",
    "    lime_explainer = LimeTabularExplainer(\n",
    "        sample_train.values,\n",
    "        feature_names=X_train.columns.tolist(),\n",
    "        class_names=['No Dementia', 'Dementia'],\n",
    "        mode='classification',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Explain 3 predictions (1 dementia, 1 no dementia, 1 borderline)\n",
    "    dementia_idx = y_test[y_test == 1].index[0]\n",
    "    no_dementia_idx = y_test[y_test == 0].index[0]\n",
    "    \n",
    "    instances = [\n",
    "        ('Dementia Case', X_test.loc[dementia_idx].values),\n",
    "        ('No Dementia Case', X_test.loc[no_dementia_idx].values)\n",
    "    ]\n",
    "    \n",
    "    for i, (label, instance) in enumerate(instances):\n",
    "        print(f\"\\nExplaining {label}...\")\n",
    "        \n",
    "        exp = lime_explainer.explain_instance(\n",
    "            instance,\n",
    "            best_model.predict_proba,\n",
    "            num_features=10\n",
    "        )\n",
    "        \n",
    "        # Save as figure\n",
    "        fig = exp.as_pyplot_figure()\n",
    "        plt.title(f\"LIME Explanation: {label}\", fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'lime_explanation_{i+1}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"✓ LIME explanation {i+1} saved\")\n",
    "    \n",
    "    print(\"\\n✓ LIME explanations complete\")\n",
    "    \n",
    "    del lime_explainer\n",
    "    gc.collect()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"⚠ LIME not installed. Run: pip install lime\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ LIME error: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ae13f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RANDOM FOREST FEATURE IMPORTANCE\n",
      "================================================================================\n",
      "\n",
      "✓ Top 15 Features by Random Forest:\n",
      "       feature  importance\n",
      "        COMMUN    0.125360\n",
      "        MEMORY    0.124515\n",
      "       CDRGLOB    0.108755\n",
      "      JUDGMENT    0.095744\n",
      "      SHOPPING    0.059369\n",
      "        ORIENT    0.059151\n",
      "      HOMEHOBB    0.049662\n",
      "      REMDATES    0.046906\n",
      "      INDEPEND    0.041238\n",
      "ADL_IMPAIRMENT    0.040434\n",
      "         BILLS    0.035710\n",
      "        TRAVEL    0.033976\n",
      "        EVENTS    0.032757\n",
      "         TAXES    0.027671\n",
      "         GAMES    0.024485\n",
      "\n",
      "✓ RF importance saved to 'rf_feature_importance.csv'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 8. FEATURE IMPORTANCE FROM RANDOM FOREST\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RANDOM FOREST FEATURE IMPORTANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "rf_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': best_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n✓ Top 15 Features by Random Forest:\")\n",
    "print(rf_importance.head(15).to_string(index=False))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(rf_importance['feature'][:20][::-1], rf_importance['importance'][:20][::-1])\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('Random Forest Feature Importance (Top 20)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('rf_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "rf_importance.to_csv('rf_feature_importance.csv', index=False)\n",
    "print(\"\\n✓ RF importance saved to 'rf_feature_importance.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9efca098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODELING & EXPLAINABILITY COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Best Model Performance:\n",
      "- Feature Set: Original\n",
      "- Model: Gradient Boosting\n",
      "- Accuracy: 0.9532\n",
      "- Precision: 0.9295\n",
      "- Recall: 0.9102\n",
      "- F1-Score: 0.9198\n",
      "- ROC-AUC: 0.9897\n",
      "\n",
      "Generated Files:\n",
      "✓ model_results.csv - All model results\n",
      "✓ shap_summary.png - SHAP feature importance\n",
      "✓ shap_dependence.png - SHAP dependence plot\n",
      "✓ shap_feature_importance.csv - SHAP importance scores\n",
      "✓ lime_explanation_1.png - LIME explanation (dementia)\n",
      "✓ lime_explanation_2.png - LIME explanation (no dementia)\n",
      "✓ rf_feature_importance.png - RF importance plot\n",
      "✓ rf_feature_importance.csv - RF importance scores\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 9. FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODELING & EXPLAINABILITY COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_result = results_df.loc[results_df['F1'].idxmax()]\n",
    "\n",
    "print(f\"\"\"\n",
    "Best Model Performance:\n",
    "- Feature Set: {best_result['Feature_Set']}\n",
    "- Model: {best_result['Model']}\n",
    "- Accuracy: {best_result['Accuracy']:.4f}\n",
    "- Precision: {best_result['Precision']:.4f}\n",
    "- Recall: {best_result['Recall']:.4f}\n",
    "- F1-Score: {best_result['F1']:.4f}\n",
    "- ROC-AUC: {best_result['AUC']:.4f}\n",
    "\n",
    "Generated Files:\n",
    "✓ model_results.csv - All model results\n",
    "✓ shap_summary.png - SHAP feature importance\n",
    "✓ shap_dependence.png - SHAP dependence plot\n",
    "✓ shap_feature_importance.csv - SHAP importance scores\n",
    "✓ lime_explanation_1.png - LIME explanation (dementia)\n",
    "✓ lime_explanation_2.png - LIME explanation (no dementia)\n",
    "✓ rf_feature_importance.png - RF importance plot\n",
    "✓ rf_feature_importance.csv - RF importance scores\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5870c25",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
